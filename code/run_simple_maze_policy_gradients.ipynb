{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy gradient iteration\n",
    "from pg import policy_gradient_iteration\n",
    "# corresponding network\n",
    "from policy_net import PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference calculation\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: -0.04\n",
      "gamma: 0.99\n",
      "starting policy gradient iteration...\n",
      "episode 500 completed, nsteps: 4, total discounted reward: -1.0891, running mean: 0.367476\n",
      "episode 1000 completed, nsteps: 5, total discounted reward: -1.11821, running mean: 0.124843\n",
      "episode 1500 completed, nsteps: 3, total discounted reward: -1.0597, running mean: 0.0151022\n",
      "episode 2000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.0106291\n",
      "episode 2500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.0294901\n",
      "episode 3000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.0999806\n",
      "episode 3500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.164274\n",
      "episode 4000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: 0.200543\n",
      "episode 4500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.254042\n",
      "frames of episode 5000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ ░ E\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "Game over!\n",
      "episode 5000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.290691\n",
      "episode 5500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.332856\n",
      "episode 6000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.346956\n",
      "episode 6500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.363844\n",
      "episode 7000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.383795\n",
      "episode 7500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.384563\n",
      "episode 8000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.415472\n",
      "episode 8500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.443567\n",
      "episode 9000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.447833\n",
      "episode 9500 completed, nsteps: 10, total discounted reward: 0.567586, running mean: 0.461469\n",
      "frames of episode 10000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 10000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.460817\n",
      "episode 10500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.490793\n",
      "episode 11000 completed, nsteps: 11, total discounted reward: 0.52191, running mean: 0.49554\n",
      "episode 11500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.495073\n",
      "episode 12000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.486536\n",
      "episode 12500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.488859\n",
      "episode 13000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.51429\n",
      "episode 13500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.519828\n",
      "episode 14000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.533187\n",
      "episode 14500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.517208\n",
      "frames of episode 15000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ ░ E\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 15000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.503375\n",
      "episode 15500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.525467\n",
      "episode 16000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.529745\n",
      "episode 16500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.537268\n",
      "episode 17000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.550263\n",
      "episode 17500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.558306\n",
      "episode 18000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.535366\n",
      "episode 18500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.536061\n",
      "episode 19000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.572652\n",
      "episode 19500 completed, nsteps: 5, total discounted reward: -1.11821, running mean: 0.556535\n",
      "frames of episode 20000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 7\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 20000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.544028\n",
      "episode 20500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.534469\n",
      "episode 21000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.530861\n",
      "episode 21500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.534869\n",
      "episode 22000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.538034\n",
      "episode 22500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.546047\n",
      "episode 23000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.537725\n",
      "episode 23500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.534814\n",
      "episode 24000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.532197\n",
      "episode 24500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.521085\n",
      "frames of episode 25000:\n",
      "step 0\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 25000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.525693\n",
      "episode 25500 completed, nsteps: 4, total discounted reward: -1.0891, running mean: 0.519085\n",
      "episode 26000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.525756\n",
      "episode 26500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.516139\n",
      "episode 27000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.517909\n",
      "episode 27500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.535469\n",
      "episode 28000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.523164\n",
      "episode 28500 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.502738\n",
      "episode 29000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.508113\n",
      "episode 29500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.516003\n",
      "frames of episode 30000:\n",
      "step 0\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 30000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.507998\n",
      "episode 30500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.526229\n",
      "episode 31000 completed, nsteps: 2, total discounted reward: -1.03, running mean: 0.5261\n",
      "episode 31500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.534231\n",
      "episode 32000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.533083\n",
      "episode 32500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.517013\n",
      "episode 33000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.50553\n",
      "episode 33500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.523308\n",
      "episode 34000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.556506\n",
      "episode 34500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.521389\n",
      "frames of episode 35000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 35000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.529855\n",
      "episode 35500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.534948\n",
      "episode 36000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.516639\n",
      "episode 36500 completed, nsteps: 5, total discounted reward: -1.11821, running mean: 0.537711\n",
      "episode 37000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.532799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 37500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.520468\n",
      "episode 38000 completed, nsteps: 6, total discounted reward: -1.14703, running mean: 0.503242\n",
      "episode 38500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.531567\n",
      "episode 39000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.537886\n",
      "episode 39500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.530623\n",
      "frames of episode 40000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ ░ E\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "Game over!\n",
      "episode 40000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.512641\n",
      "episode 40500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.528818\n",
      "episode 41000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.517344\n",
      "episode 41500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.51134\n",
      "episode 42000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.534914\n",
      "episode 42500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.532181\n",
      "episode 43000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.533439\n",
      "episode 43500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.551538\n",
      "episode 44000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.54949\n",
      "episode 44500 completed, nsteps: 5, total discounted reward: -1.11821, running mean: 0.537669\n",
      "frames of episode 45000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 45000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.536726\n",
      "episode 45500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.531786\n",
      "episode 46000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.52458\n",
      "episode 46500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.526313\n",
      "episode 47000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.534235\n",
      "episode 47500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.522854\n",
      "episode 48000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.536868\n",
      "episode 48500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.54082\n",
      "episode 49000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.52837\n",
      "episode 49500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.519508\n",
      "frames of episode 50000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 50000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.497667\n",
      "policy iteration completed after 3 iterations\n",
      "optimal value function average: 0.5476936325612626\n",
      "policy (most likely action, for all possible ghost locations):\n",
      "→ → → E\n",
      "↑ █ ↑ F\n",
      "↑ → ↑ ←\n",
      "number of deviations from reference: 1\n"
     ]
    }
   ],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04\n",
    "print('r:', r)\n",
    "\n",
    "e = MazeEnv('simple_maze.txt', r)\n",
    "\n",
    "gamma = 0.99\n",
    "print('gamma:', gamma)\n",
    "\n",
    "net = PolicyNet(e.observation(0).size, e.num_actions)\n",
    "\n",
    "print('starting policy gradient iteration...')\n",
    "net = policy_gradient_iteration(net, e, gamma, nepisodes=50000)\n",
    "# obtain policy from network: most likely action for each state\n",
    "pol = np.zeros(e.num_states, dtype=int)\n",
    "for s in range(e.num_states):\n",
    "    x = e.observation(s).reshape(-1)\n",
    "    aprob = net.evaluate(x[None, :])[0]\n",
    "    pol[s] = np.argmax(aprob)\n",
    "\n",
    "# reference optimal policy\n",
    "pref = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "# corresponding value function\n",
    "uref = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pref)\n",
    "# omit \"game over\" from average\n",
    "umean = np.mean(uref[:-1])\n",
    "print('optimal value function average:', umean)\n",
    "\n",
    "# compare policy with reference\n",
    "print('policy (most likely action, for all possible ghost locations):')\n",
    "print(e.draw_policy(pol))\n",
    "print('number of deviations from reference:', np.sum((pol - pref) * e.pmask != 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
