{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative algorithms for MDPs, demonstrated via a simple Pac-Man game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves within a maze and tries to reach an exit field with a positive reward, while not being captured by the ghost in the maze. The agent's locomotion rules agree with *plain_maze*, and the ghost moves randomly by one field per step, with preferred direction towards the agent. The state space consists of all possible agent and ghost locations, plus a final \"game over\" state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative algorithms and utility functions for Markov Decision Processes (MDPs)\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeGhostEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# S: start, X: inaccessible, E: exit with reward +1, F: exit with reward -1\n",
      "..XE\n",
      ".X..\n",
      ".X.F\n",
      "S...\n"
     ]
    }
   ],
   "source": [
    "# show description of maze (grid world) geometry\n",
    "with open('maze_geometry2.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "e = MazeGhostEnv('maze_geometry2.txt', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value iteration with epsilon=1e-14 completed after 154 iterations\n",
      "value function for ghost at first accessible location:\n",
      "[[-1.223 -1.242    nan  1.   ]\n",
      " [-1.235    nan  0.824  0.924]\n",
      " [-1.141    nan  0.563 -1.   ]\n",
      " [-2.     0.035  0.302  0.064]]\n"
     ]
    }
   ],
   "source": [
    "# perform value iteration\n",
    "u = mdp.value_iteration(e.tprob, e.rewards, gamma)\n",
    "print('value function for ghost at first accessible location:')\n",
    "if hasattr(np, 'printoptions'):\n",
    "    with np.printoptions(precision=3):\n",
    "        print(e.maze_array(u[:len(e.locs)]))\n",
    "else:\n",
    "    print(e.maze_array(u[:len(e.locs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy (all possible ghost locations):\n",
      "↓ ← █ E\n",
      "↑ █ → ↑\n",
      "↓ █ ↑ F\n",
      "G → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↑ █ ↑ F\n",
      "→ G ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "← → G ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ ↑ ↑ G\n",
      "\n",
      "↑ ← █ E\n",
      "↓ █ → ↑\n",
      "G █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ G F\n",
      "→ ↑ ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ ↑ ↑\n",
      "↓ █ ↑ G\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ → █ E\n",
      "G █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ G ↑\n",
      "↓ █ ↓ F\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → G\n",
      "↓ █ ← F\n",
      "→ → ↑ ↓\n",
      "\n",
      "G ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ G █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ G\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n"
     ]
    }
   ],
   "source": [
    "# optimal policy corresponding to u;\n",
    "# in some cases the agent tries to \"jump over\" the ghost\n",
    "pol = mdp.policy_from_utility(e.tprob, u)\n",
    "print('optimal policy (all possible ghost locations):')\n",
    "print(e.draw_policy(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility from policy consistency check error: 3.046221841855575e-15\n"
     ]
    }
   ],
   "source": [
    "# consistency check\n",
    "if gamma < 1:\n",
    "    upol = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pol)\n",
    "    uerr = np.linalg.norm(upol - u)\n",
    "    print('utility from policy consistency check error:', uerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration completed after 4 iterations\n",
      "policy iteration consistency check error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# alternative: policy iteration\n",
    "if gamma < 1:\n",
    "    pal = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "    palerr = np.linalg.norm((pal - pol) * e.pmask)\n",
    "    print('policy iteration consistency check error:', palerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value iteration with epsilon=1e-14 completed after 139 iterations\n"
     ]
    }
   ],
   "source": [
    "# Q-value function\n",
    "Q = mdp.q_iteration(e.tprob, e.rewards, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility from Q-value consistency check error: 1.1012004949980631e-14\n"
     ]
    }
   ],
   "source": [
    "# can obtain utility from Q-value function\n",
    "uQ = mdp.utility_from_qvalue(Q)\n",
    "uQerr = np.linalg.norm(u - uQ)\n",
    "print('utility from Q-value consistency check error:', uQerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy from Q-value consistency check error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# can obtain policy from Q-value function\n",
    "pQ = mdp.policy_from_qvalue(Q)\n",
    "pQerr = np.linalg.norm((pQ - pol) * e.pmask)\n",
    "print('policy from Q-value consistency check error:', pQerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "reward: -0.04\n",
      "G ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "G ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "G █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "G █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "G ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "G █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 8\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 9\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 10\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "G █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "cumulative discounted reward (gamma = 0.99): 0.5219103750440224\n"
     ]
    }
   ],
   "source": [
    "# play a game\n",
    "e.play(pol, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
