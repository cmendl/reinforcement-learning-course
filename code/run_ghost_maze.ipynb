{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative algorithms and utility functions for Markov Decision Processes (MDPs)\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeGhostEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: -0.04\n",
      "gamma: 0.99\n",
      "value iteration with epsilon=1e-14 completed after 154 iterations\n",
      "value function for ghost at first accessible location:\n",
      "[[-1.223 -1.242    nan  1.   ]\n",
      " [-1.235    nan  0.824  0.924]\n",
      " [-1.141    nan  0.563 -1.   ]\n",
      " [-2.     0.035  0.302  0.064]]\n",
      "optimal policy (all possible ghost locations):\n",
      "↓ ← █ E\n",
      "↑ █ → ↑\n",
      "↓ █ ↑ F\n",
      "G → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↑ █ ↑ F\n",
      "→ G ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "← → G ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ ↑ ↑ G\n",
      "\n",
      "↑ ← █ E\n",
      "↓ █ → ↑\n",
      "G █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ G F\n",
      "→ ↑ ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ ↑ ↑\n",
      "↓ █ ↑ G\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ → █ E\n",
      "G █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ G ↑\n",
      "↓ █ ↓ F\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → G\n",
      "↓ █ ← F\n",
      "→ → ↑ ↓\n",
      "\n",
      "G ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ G █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ G\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "utility from policy consistency check error: 3.046221841855575e-15\n",
      "policy iteration completed after 4 iterations\n",
      "policy iteration consistency check error: 0.0\n",
      "Q-value iteration with epsilon=1e-14 completed after 139 iterations\n",
      "utility from Q-value consistency check error: 1.1012004949980631e-14\n",
      "policy from Q-value consistency check error: 0.0\n",
      "playing a game...\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ G ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 8\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 9\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 10\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "cumulative discounted reward (gamma = 0.99): 0.5219103750440224\n"
     ]
    }
   ],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04\n",
    "print('r:', r)\n",
    "\n",
    "e = MazeGhostEnv('ghost_maze.txt', r)\n",
    "\n",
    "gamma = 0.99\n",
    "print('gamma:', gamma)\n",
    "\n",
    "# perform value iteration\n",
    "u = mdp.value_iteration(e.tprob, e.rewards, gamma)\n",
    "print('value function for ghost at first accessible location:')\n",
    "if hasattr(np, 'printoptions'):\n",
    "    with np.printoptions(precision=3):\n",
    "        print(e.maze_array(u[:len(e.locs)]))\n",
    "else:\n",
    "    print(e.maze_array(u[:len(e.locs)]))\n",
    "\n",
    "# optimal policy corresponding to u\n",
    "pol = mdp.policy_from_utility(e.tprob, u)\n",
    "print('optimal policy (all possible ghost locations):')\n",
    "print(e.draw_policy(pol))\n",
    "\n",
    "# consistency check\n",
    "if gamma < 1:\n",
    "    upol = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pol)\n",
    "    uerr = np.linalg.norm(upol - u)\n",
    "    print('utility from policy consistency check error:', uerr)\n",
    "\n",
    "# alternative: policy iteration\n",
    "if gamma < 1:\n",
    "    pal = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "    palerr = np.linalg.norm((pal - pol) * e.pmask)\n",
    "    print('policy iteration consistency check error:', palerr)\n",
    "\n",
    "# Q-value function\n",
    "Q = mdp.q_iteration(e.tprob, e.rewards, gamma)\n",
    "\n",
    "uQ = mdp.utility_from_qvalue(Q)\n",
    "uQerr = np.linalg.norm(u - uQ)\n",
    "print('utility from Q-value consistency check error:', uQerr)\n",
    "\n",
    "pQ = mdp.policy_from_qvalue(Q)\n",
    "pQerr = np.linalg.norm((pQ - pol) * e.pmask)\n",
    "print('policy from Q-value consistency check error:', pQerr)\n",
    "\n",
    "# play a game\n",
    "print('playing a game...')\n",
    "e.play(pol, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
