{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradients for MDPs, demonstrated via a grid world toy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves within a maze and tries to reach an exit field with a positive reward, while not being captured by the ghost in the maze. The agent's locomotion rules agree with *plain_maze*, and the ghost moves randomly by one field per step, with preferred direction towards the agent. The state space consists of all possible agent and ghost locations, plus a final \"game over\" state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy gradient iteration\n",
    "from pg import policy_gradient_iteration\n",
    "# corresponding network\n",
    "from policy_net import PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative algorithms for reference calculation\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# S: start, X: inaccessible, E: exit with reward +1, F: exit with reward -1\n",
      "...E\n",
      ".X.F\n",
      "S...\n"
     ]
    }
   ],
   "source": [
    "# show description of maze (grid world) geometry\n",
    "with open('maze_geometry.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "e = MazeEnv('maze_geometry.txt', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy neural network\n",
    "net = PolicyNet(e.observation(0).size, e.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting policy gradient iteration...\n",
      "episode 500 completed, nsteps: 1, total discounted reward: -1, running mean: -0.997288\n",
      "episode 1000 completed, nsteps: 18, total discounted reward: -1.47117, running mean: -0.716186\n",
      "episode 1500 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -0.544335\n",
      "episode 2000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.364709\n",
      "episode 2500 completed, nsteps: 12, total discounted reward: -1.31399, running mean: -0.193828\n",
      "episode 3000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.0585713\n",
      "episode 3500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.0569061\n",
      "episode 4000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.123769\n",
      "episode 4500 completed, nsteps: 10, total discounted reward: -1.25945, running mean: 0.1676\n",
      "frames of episode 5000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 8\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 5000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.215223\n",
      "episode 5500 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.260616\n",
      "episode 6000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.292586\n",
      "episode 6500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.32007\n",
      "episode 7000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.331968\n",
      "episode 7500 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.339896\n",
      "episode 8000 completed, nsteps: 1, total discounted reward: -1, running mean: 0.370322\n",
      "episode 8500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.374996\n",
      "episode 9000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.397429\n",
      "episode 9500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.408746\n",
      "frames of episode 10000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 10000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.410855\n",
      "episode 10500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.435594\n",
      "episode 11000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.432184\n",
      "episode 11500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.440276\n",
      "episode 12000 completed, nsteps: 11, total discounted reward: 0.52191, running mean: 0.461613\n",
      "episode 12500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.46885\n",
      "episode 13000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.486295\n",
      "episode 13500 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.500067\n",
      "episode 14000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.494478\n",
      "episode 14500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.492995\n",
      "frames of episode 15000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 15000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.501549\n",
      "episode 15500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.500161\n",
      "episode 16000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.521317\n",
      "episode 16500 completed, nsteps: 2, total discounted reward: -1.03, running mean: 0.526624\n",
      "episode 17000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.526054\n",
      "episode 17500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.515001\n",
      "episode 18000 completed, nsteps: 2, total discounted reward: -1.03, running mean: 0.511353\n",
      "episode 18500 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.503951\n",
      "episode 19000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.515552\n",
      "episode 19500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.522733\n",
      "frames of episode 20000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 20000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.51819\n",
      "episode 20500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.503637\n",
      "episode 21000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.509053\n",
      "episode 21500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.538406\n",
      "episode 22000 completed, nsteps: 2, total discounted reward: -1.03, running mean: 0.569933\n",
      "episode 22500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.563098\n",
      "episode 23000 completed, nsteps: 4, total discounted reward: -1.0891, running mean: 0.538159\n",
      "episode 23500 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.523519\n",
      "episode 24000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.539466\n",
      "episode 24500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.537923\n",
      "frames of episode 25000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 25000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.544565\n",
      "episode 25500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.540707\n",
      "episode 26000 completed, nsteps: 7, total discounted reward: -1.17556, running mean: 0.50993\n",
      "episode 26500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.507115\n",
      "episode 27000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.510311\n",
      "episode 27500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.524431\n",
      "episode 28000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.530056\n",
      "episode 28500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.524737\n",
      "episode 29000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.518735\n",
      "episode 29500 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.513044\n",
      "frames of episode 30000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ←\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 8\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 30000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: 0.509384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 30500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.504695\n",
      "episode 31000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.505097\n",
      "episode 31500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.514809\n",
      "episode 32000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.513702\n",
      "episode 32500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.525957\n",
      "episode 33000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.532842\n",
      "episode 33500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.517434\n",
      "episode 34000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.521434\n",
      "episode 34500 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.536683\n",
      "frames of episode 35000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 35000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.537496\n",
      "episode 35500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.520137\n",
      "episode 36000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.533\n",
      "episode 36500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.507902\n",
      "episode 37000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.507566\n",
      "episode 37500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.490995\n",
      "episode 38000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.515329\n",
      "episode 38500 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.510894\n",
      "episode 39000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.511222\n",
      "episode 39500 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.50669\n",
      "frames of episode 40000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 40000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.538773\n",
      "episode 40500 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.532779\n",
      "episode 41000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.534428\n",
      "episode 41500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.528929\n",
      "episode 42000 completed, nsteps: 2, total discounted reward: 0.95, running mean: 0.513305\n",
      "episode 42500 completed, nsteps: 1, total discounted reward: 1, running mean: 0.517054\n",
      "episode 43000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.49133\n",
      "episode 43500 completed, nsteps: 7, total discounted reward: 0.707401, running mean: 0.490851\n",
      "episode 44000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: 0.512719\n",
      "episode 44500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.511662\n",
      "frames of episode 45000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ░ E\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "☺ ░ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ☺ ░ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 45000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.511131\n",
      "episode 45500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.529787\n",
      "episode 46000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: 0.54377\n",
      "episode 46500 completed, nsteps: 13, total discounted reward: 0.431924, running mean: 0.547809\n",
      "episode 47000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.545629\n",
      "episode 47500 completed, nsteps: 6, total discounted reward: 0.75495, running mean: 0.527264\n",
      "episode 48000 completed, nsteps: 1, total discounted reward: 1, running mean: 0.540507\n",
      "episode 48500 completed, nsteps: 4, total discounted reward: 0.851495, running mean: 0.521997\n",
      "episode 49000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: 0.51204\n",
      "episode 49500 completed, nsteps: 1, total discounted reward: -1, running mean: 0.542081\n",
      "frames of episode 50000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ ☺ E\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: 1.0\n",
      "░ ░ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 50000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: 0.528301\n"
     ]
    }
   ],
   "source": [
    "print('starting policy gradient iteration...')\n",
    "net = policy_gradient_iteration(net, e, gamma, nepisodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain policy from network: most likely action for each state\n",
    "pol = np.zeros(e.num_states, dtype=int)\n",
    "for s in range(e.num_states):\n",
    "    x = e.observation(s).reshape(-1)\n",
    "    aprob = net.evaluate(x[None, :])[0]\n",
    "    pol[s] = np.argmax(aprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration completed after 3 iterations\n",
      "optimal value function average: 0.5476936325612626\n"
     ]
    }
   ],
   "source": [
    "# reference optimal policy\n",
    "pref = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "# corresponding value function\n",
    "uref = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pref)\n",
    "# omit \"game over\" from average\n",
    "umean = np.mean(uref[:-1])\n",
    "print('optimal value function average:', umean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy (most likely action):\n",
      "→ → → E\n",
      "↑ █ ↑ F\n",
      "↑ → ↑ ←\n",
      "policy reference solution:\n",
      "→ → → E\n",
      "↑ █ ↑ F\n",
      "↑ ← ↑ ←\n",
      "number of deviations from reference: 1\n"
     ]
    }
   ],
   "source": [
    "# compare policy with reference\n",
    "print('policy (most likely action):')\n",
    "print(e.draw_policy(pol))\n",
    "print('policy reference solution:')\n",
    "print(e.draw_policy(pref))\n",
    "print('number of deviations from reference:', np.sum((pol - pref) * e.pmask != 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
