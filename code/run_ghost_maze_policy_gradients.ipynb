{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy gradient iteration\n",
    "from pg import policy_gradient_iteration\n",
    "# corresponding network\n",
    "from policy_net import PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeGhostEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference calculation\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: -0.04\n",
      "gamma: 0.99\n",
      "starting policy gradient iteration...\n",
      "episode 1000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -1.89397\n",
      "episode 2000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -1.60206\n",
      "episode 3000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -1.44134\n",
      "episode 4000 completed, nsteps: 1, total discounted reward: -1, running mean: -1.3422\n",
      "episode 5000 completed, nsteps: 34, total discounted reward: -2.56454, running mean: -1.18577\n",
      "episode 6000 completed, nsteps: 9, total discounted reward: -2.15451, running mean: -1.11136\n",
      "episode 7000 completed, nsteps: 19, total discounted reward: -2.33097, running mean: -1.03203\n",
      "episode 8000 completed, nsteps: 12, total discounted reward: -1.31399, running mean: -0.929293\n",
      "episode 9000 completed, nsteps: 4, total discounted reward: -2.0594, running mean: -0.897773\n",
      "frames of episode 10000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ G\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ G\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 10000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: -0.892153\n",
      "episode 11000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: -0.812111\n",
      "episode 12000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.758113\n",
      "episode 13000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.729161\n",
      "episode 14000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.72777\n",
      "episode 15000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.689151\n",
      "episode 16000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.725099\n",
      "episode 17000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.634922\n",
      "episode 18000 completed, nsteps: 21, total discounted reward: -2.36419, running mean: -0.645361\n",
      "episode 19000 completed, nsteps: 7, total discounted reward: -1.17556, running mean: -0.65287\n",
      "frames of episode 20000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "G ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "G █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 20000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.659957\n",
      "episode 21000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.605715\n",
      "episode 22000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.623568\n",
      "episode 23000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.595527\n",
      "episode 24000 completed, nsteps: 7, total discounted reward: -2.11704, running mean: -0.561005\n",
      "episode 25000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.571694\n",
      "episode 26000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.507789\n",
      "episode 27000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.550547\n",
      "episode 28000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.536428\n",
      "episode 29000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.574542\n",
      "frames of episode 30000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "Game over!\n",
      "episode 30000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.600913\n",
      "episode 31000 completed, nsteps: 9, total discounted reward: -2.15451, running mean: -0.590182\n",
      "episode 32000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.560601\n",
      "episode 33000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.552279\n",
      "episode 34000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.570851\n",
      "episode 35000 completed, nsteps: 6, total discounted reward: -2.09802, running mean: -0.542163\n",
      "episode 36000 completed, nsteps: 4, total discounted reward: -1.0891, running mean: -0.521997\n",
      "episode 37000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: -0.528727\n",
      "episode 38000 completed, nsteps: 11, total discounted reward: 0.52191, running mean: -0.544041\n",
      "episode 39000 completed, nsteps: 11, total discounted reward: 0.52191, running mean: -0.53074\n",
      "frames of episode 40000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ☺ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "☺ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "☺ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "☺ █ G F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 7\n",
      "reward: -2.0\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 40000 completed, nsteps: 8, total discounted reward: -2.13587, running mean: -0.497929\n",
      "episode 41000 completed, nsteps: 9, total discounted reward: -2.15451, running mean: -0.534489\n",
      "episode 42000 completed, nsteps: 12, total discounted reward: 0.476691, running mean: -0.514505\n",
      "episode 43000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.531101\n",
      "episode 44000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.462206\n",
      "episode 45000 completed, nsteps: 5, total discounted reward: -2.07881, running mean: -0.460239\n",
      "episode 46000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.464836\n",
      "episode 47000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.472172\n",
      "episode 48000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.486302\n",
      "episode 49000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: -0.481625\n",
      "frames of episode 50000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -2.0\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 50000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.486113\n",
      "episode 51000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: -0.46845\n",
      "episode 52000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.47499\n",
      "episode 53000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: -0.472212\n",
      "episode 54000 completed, nsteps: 15, total discounted reward: -1.39376, running mean: -0.437979\n",
      "episode 55000 completed, nsteps: 14, total discounted reward: -2.24496, running mean: -0.431819\n",
      "episode 56000 completed, nsteps: 5, total discounted reward: -2.07881, running mean: -0.473944\n",
      "episode 57000 completed, nsteps: 13, total discounted reward: 0.431924, running mean: -0.461526\n",
      "episode 58000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.526482\n",
      "episode 59000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.446783\n",
      "frames of episode 60000:\n",
      "step 0\n",
      "reward: -2.0\n",
      "░ G █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 60000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.466017\n",
      "episode 61000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.40894\n",
      "episode 62000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.428345\n",
      "episode 63000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.461299\n",
      "episode 64000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: -0.478958\n",
      "episode 65000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.490675\n",
      "episode 66000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: -0.456306\n",
      "episode 67000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.467513\n",
      "episode 68000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.435008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 69000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.459147\n",
      "frames of episode 70000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ☺ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ☺ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ☺ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "☺ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "☺ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 8\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 9\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 10\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 11\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 12\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 13\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 14\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ G\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 15\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 16\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 17\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 70000 completed, nsteps: 18, total discounted reward: 0.214716, running mean: -0.464907\n",
      "episode 71000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.488572\n",
      "episode 72000 completed, nsteps: 13, total discounted reward: 0.431924, running mean: -0.436736\n",
      "episode 73000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.440938\n",
      "episode 74000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.436811\n",
      "episode 75000 completed, nsteps: 15, total discounted reward: -2.26251, running mean: -0.433429\n",
      "episode 76000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.454202\n",
      "episode 77000 completed, nsteps: 12, total discounted reward: 0.476691, running mean: -0.406296\n",
      "episode 78000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.396007\n",
      "episode 79000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.473123\n",
      "frames of episode 80000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 80000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.45566\n",
      "episode 81000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.383078\n",
      "episode 82000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.388508\n",
      "episode 83000 completed, nsteps: 7, total discounted reward: -1.17556, running mean: -0.393372\n",
      "episode 84000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.429072\n",
      "episode 85000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.423974\n",
      "episode 86000 completed, nsteps: 7, total discounted reward: -1.17556, running mean: -0.416767\n",
      "episode 87000 completed, nsteps: 12, total discounted reward: -2.20932, running mean: -0.403725\n",
      "episode 88000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.442251\n",
      "episode 89000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.450196\n",
      "frames of episode 90000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "☺ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "☺ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 8\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ G\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 9\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 10\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ G ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 11\n",
      "reward: -1.0\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G ☺\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 90000 completed, nsteps: 12, total discounted reward: -1.31399, running mean: -0.433956\n",
      "episode 91000 completed, nsteps: 9, total discounted reward: -1.23177, running mean: -0.437284\n",
      "episode 92000 completed, nsteps: 10, total discounted reward: -2.17297, running mean: -0.455297\n",
      "episode 93000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.449975\n",
      "episode 94000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.452666\n",
      "episode 95000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: -0.437449\n",
      "episode 96000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.401981\n",
      "episode 97000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.482038\n",
      "episode 98000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.499181\n",
      "episode 99000 completed, nsteps: 6, total discounted reward: -1.14703, running mean: -0.53017\n",
      "frames of episode 100000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 100000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.416527\n",
      "policy iteration completed after 4 iterations\n",
      "optimal value function average: -0.28615611539027547\n",
      "policy (most likely action, for all possible ghost locations):\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "G → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ G ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → G ←\n",
      "\n",
      "↓ ↑ █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ G\n",
      "\n",
      "↑ ← █ E\n",
      "→ █ → ↑\n",
      "G █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ G F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ G\n",
      "→ → ↑ ←\n",
      "\n",
      "↑ ← █ E\n",
      "G █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ G ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → G\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "G ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ G █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ G\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "number of deviations from reference: 17\n"
     ]
    }
   ],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04\n",
    "print('r:', r)\n",
    "\n",
    "e = MazeGhostEnv('ghost_maze.txt', r)\n",
    "\n",
    "gamma = 0.99\n",
    "print('gamma:', gamma)\n",
    "\n",
    "net = PolicyNet(e.observation(0).size, e.num_actions)\n",
    "\n",
    "print('starting policy gradient iteration...')\n",
    "net = policy_gradient_iteration(net, e, gamma)\n",
    "# obtain policy from network: most likely action for each state\n",
    "pol = np.zeros(e.num_states, dtype=int)\n",
    "for s in range(e.num_states):\n",
    "    x = e.observation(s).reshape(-1)\n",
    "    aprob = net.evaluate(x[None, :])[0]\n",
    "    pol[s] = np.argmax(aprob)\n",
    "\n",
    "# reference optimal policy\n",
    "pref = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "# corresponding value function\n",
    "uref = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pref)\n",
    "# omit \"game over\" from average\n",
    "umean = np.mean(uref[:-1])\n",
    "print('optimal value function average:', umean)\n",
    "\n",
    "# compare policy with reference\n",
    "print('policy (most likely action, for all possible ghost locations):')\n",
    "print(e.draw_policy(pol))\n",
    "print('number of deviations from reference:', np.sum((pol - pref) * e.pmask != 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
