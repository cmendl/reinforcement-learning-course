{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative algorithms and utility functions for Markov Decision Processes (MDPs)\n",
    "import mdp\n",
    "# environment\n",
    "from env import MazeGhostEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(e, gamma, eta, nepisodes=100000):\n",
    "    \"\"\"\n",
    "    Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        e:         environment\n",
    "        gamma:     discount factor\n",
    "        eta:       learning rate\n",
    "        nepisodes: number of episodes (to-be played games)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Q-value function\n",
    "    \"\"\"\n",
    "    Q = np.zeros((e.num_states, e.num_actions))\n",
    "    for i in range(nepisodes):\n",
    "        # initial (uniformly random) state and reward\n",
    "        s = np.random.randint(e.num_states)\n",
    "        r = e.rewards[s]\n",
    "        game_over = False\n",
    "        while not game_over:\n",
    "            # choose an action based on softmax probability derived from Q[s, :]\n",
    "            pqs = np.exp(Q[s, :])\n",
    "            pqs /= np.sum(pqs)\n",
    "            a = np.random.choice(e.num_actions, p=pqs)\n",
    "            # transition to next state\n",
    "            (snext, rnext, game_over) = e.step(s, a)\n",
    "            #\n",
    "            # TODO: Update Q-value function for state 's' and action 'a'.\n",
    "            #\n",
    "            s = snext\n",
    "            r = rnext\n",
    "        if (i+1) % (nepisodes//10) == 0:\n",
    "            print('{}%'.format(100*(i+1)//nepisodes))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "#\n",
    "#def q_learn(e, gamma, eta, nepisodes=100000):\n",
    "#    \"\"\"\n",
    "#    Q-learning algorithm.\n",
    "#\n",
    "#    Args:\n",
    "#        e:         environment\n",
    "#        gamma:     discount factor\n",
    "#        eta:       learning rate\n",
    "#        nepisodes: number of episodes (to-be played games)\n",
    "#\n",
    "#    Returns:\n",
    "#        numpy.ndarray: Q-value function\n",
    "#    \"\"\"\n",
    "#    Q = np.zeros((e.num_states, e.num_actions))\n",
    "#    for i in range(nepisodes):\n",
    "#        # initial (uniformly random) state and reward\n",
    "#        s = np.random.randint(e.num_states)\n",
    "#        r = e.rewards[s]\n",
    "#        game_over = False\n",
    "#        while not game_over:\n",
    "#            # choose an action based on softmax probability derived from Q[s, :]\n",
    "#            pqs = np.exp(Q[s, :])\n",
    "#            pqs /= np.sum(pqs)\n",
    "#            a = np.random.choice(e.num_actions, p=pqs)\n",
    "#            # transition to next state\n",
    "#            (snext, rnext, game_over) = e.step(s, a)\n",
    "#            # update Q-value function\n",
    "#            Q[s, a] += eta * (r + gamma * np.amax(Q[snext, :]) - Q[s, a])\n",
    "#            s = snext\n",
    "#            r = rnext\n",
    "#        if (i+1) % (nepisodes//10) == 0:\n",
    "#            print('{}%'.format(100*(i+1)//nepisodes))\n",
    "#    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# S: start, X: inaccessible, E: exit with reward +1, F: exit with reward -1\n",
      "..XE\n",
      ".X..\n",
      ".X.F\n",
      "S...\n"
     ]
    }
   ],
   "source": [
    "# show maze description\n",
    "with open('ghost_maze.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: -0.04\n",
      "gamma: 0.99\n",
      "learning rate eta: 0.01\n",
      "starting Q-learning algorithm...\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Q-value iteration with epsilon=1e-14 completed after 139 iterations\n",
      "|Qlearn - Qref|: 0.30784180848870796\n",
      "Qlearn policy (all possible ghost locations):\n",
      "↓ ← █ E\n",
      "← █ → ↑\n",
      "↓ █ ↑ F\n",
      "G → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "→ █ ↑ F\n",
      "↑ G ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "← → G ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ ↑ ↑ G\n",
      "\n",
      "↑ ← █ E\n",
      "↓ █ → ↑\n",
      "G █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ G F\n",
      "→ ↓ ↑ ↓\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ ↑ ↑\n",
      "↓ █ ↑ G\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ ↑ █ E\n",
      "G █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ G ↑\n",
      "↓ █ ↓ F\n",
      "→ → ↓ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → G\n",
      "↓ █ ← F\n",
      "→ → ↑ ↓\n",
      "\n",
      "G ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ G █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ G\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "number of deviations from reference: 5\n"
     ]
    }
   ],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04\n",
    "print('r:', r)\n",
    "\n",
    "e = MazeGhostEnv('ghost_maze.txt', r)\n",
    "\n",
    "gamma = 0.99\n",
    "print('gamma:', gamma)\n",
    "\n",
    "eta = 0.01\n",
    "print('learning rate eta:', eta)\n",
    "\n",
    "# perform Q-learning\n",
    "print('starting Q-learning algorithm...')\n",
    "Qlearn = q_learn(e, gamma, eta)\n",
    "pol = mdp.policy_from_qvalue(Qlearn)\n",
    "\n",
    "# reference Q-value function and policy\n",
    "Qref = mdp.q_iteration(e.tprob, e.rewards, gamma)\n",
    "pref = mdp.policy_from_qvalue(Qref)\n",
    "\n",
    "# compare Q-value function with reference\n",
    "print('|Qlearn - Qref|:', np.linalg.norm((Qlearn - Qref).reshape(-1), np.inf))\n",
    "\n",
    "# compare policy with reference\n",
    "print('Qlearn policy (all possible ghost locations):')\n",
    "print(e.draw_policy(pol))\n",
    "print('number of deviations from reference:', np.sum((pol - pref) * e.pmask != 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
