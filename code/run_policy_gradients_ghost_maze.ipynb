{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradients for MDPs, demonstrated via a simple Pac-Man game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves within a maze and tries to reach an exit field with a positive reward, while not being captured by the ghost in the maze. The agent's locomotion rules agree with *plain_maze*, and the ghost moves randomly by one field per step, with preferred direction towards the agent. The state space consists of all possible agent and ghost locations, plus a final \"game over\" state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy gradient iteration\n",
    "from pg import policy_gradient_iteration\n",
    "# corresponding network\n",
    "from policy_net import PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "from env import MazeGhostEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative algorithms for reference calculation\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# S: start, X: inaccessible, E: exit with reward +1, F: exit with reward -1\n",
      "..XE\n",
      ".X..\n",
      ".X.F\n",
      "S...\n"
     ]
    }
   ],
   "source": [
    "# show description of maze (grid world) geometry\n",
    "with open('maze_geometry2.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reward\" on regular fields\n",
    "r = -0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "e = MazeGhostEnv('maze_geometry2.txt', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy neural network\n",
    "net = PolicyNet(e.observation(0).size, e.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting policy gradient iteration...\n",
      "episode 1000 completed, nsteps: 32, total discounted reward: -2.53539, running mean: -1.73258\n",
      "episode 2000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -1.56868\n",
      "episode 3000 completed, nsteps: 14, total discounted reward: -2.24496, running mean: -1.42507\n",
      "episode 4000 completed, nsteps: 4, total discounted reward: -1.0891, running mean: -1.33853\n",
      "episode 5000 completed, nsteps: 1, total discounted reward: -1, running mean: -1.24636\n",
      "episode 6000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -1.17593\n",
      "episode 7000 completed, nsteps: 4, total discounted reward: -2.0594, running mean: -1.08287\n",
      "episode 8000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -1.01093\n",
      "episode 9000 completed, nsteps: 14, total discounted reward: -2.24496, running mean: -0.947025\n",
      "frames of episode 10000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ☺\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ↓\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ↓\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: -1.0\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "Game over!\n",
      "episode 10000 completed, nsteps: 6, total discounted reward: -1.14703, running mean: -0.891419\n",
      "episode 11000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.822586\n",
      "episode 12000 completed, nsteps: 15, total discounted reward: 0.343729, running mean: -0.778689\n",
      "episode 13000 completed, nsteps: 20, total discounted reward: -1.52149, running mean: -0.740592\n",
      "episode 14000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.735686\n",
      "episode 15000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.710399\n",
      "episode 16000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.706457\n",
      "episode 17000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -0.690667\n",
      "episode 18000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.69298\n",
      "episode 19000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.676954\n",
      "frames of episode 20000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "G ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "G ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "G ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: ←\n",
      "_____________\n",
      "Game over!\n",
      "episode 20000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.666468\n",
      "episode 21000 completed, nsteps: 19, total discounted reward: 0.172569, running mean: -0.657301\n",
      "episode 22000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.644788\n",
      "episode 23000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.634633\n",
      "episode 24000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.633783\n",
      "episode 25000 completed, nsteps: 8, total discounted reward: -2.13587, running mean: -0.637813\n",
      "episode 26000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.621121\n",
      "episode 27000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.604541\n",
      "episode 28000 completed, nsteps: 11, total discounted reward: -2.19124, running mean: -0.589304\n",
      "episode 29000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.563397\n",
      "frames of episode 30000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "G ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "G ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ☺\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -1.0\n",
      "G ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 30000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -0.564079\n",
      "episode 31000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.533372\n",
      "episode 32000 completed, nsteps: 2, total discounted reward: -1.03, running mean: -0.553895\n",
      "episode 33000 completed, nsteps: 3, total discounted reward: -2.0398, running mean: -0.539852\n",
      "episode 34000 completed, nsteps: 6, total discounted reward: -2.09802, running mean: -0.553946\n",
      "episode 35000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.559801\n",
      "episode 36000 completed, nsteps: 7, total discounted reward: -2.11704, running mean: -0.483755\n",
      "episode 37000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.537612\n",
      "episode 38000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.546929\n",
      "episode 39000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.571291\n",
      "frames of episode 40000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "G █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "G █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "G █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "G █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "G █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 6\n",
      "reward: -1.0\n",
      "░ ░ █ E\n",
      "G █ ░ ░\n",
      "░ █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 40000 completed, nsteps: 7, total discounted reward: -1.17556, running mean: -0.540554\n",
      "episode 41000 completed, nsteps: 16, total discounted reward: -2.27988, running mean: -0.569753\n",
      "episode 42000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.577254\n",
      "episode 43000 completed, nsteps: 4, total discounted reward: -1.0891, running mean: -0.545573\n",
      "episode 44000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.541734\n",
      "episode 45000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.510429\n",
      "episode 46000 completed, nsteps: 7, total discounted reward: -2.11704, running mean: -0.523221\n",
      "episode 47000 completed, nsteps: 6, total discounted reward: -2.09802, running mean: -0.533681\n",
      "episode 48000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.52853\n",
      "episode 49000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.476962\n",
      "frames of episode 50000:\n",
      "step 0\n",
      "reward: -2.0\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "Game over!\n",
      "episode 50000 completed, nsteps: 1, total discounted reward: -2, running mean: -0.462171\n",
      "episode 51000 completed, nsteps: 11, total discounted reward: -2.19124, running mean: -0.536564\n",
      "episode 52000 completed, nsteps: 3, total discounted reward: -1.0597, running mean: -0.516356\n",
      "episode 53000 completed, nsteps: 12, total discounted reward: 0.476691, running mean: -0.4548\n",
      "episode 54000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.464333\n",
      "episode 55000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.450494\n",
      "episode 56000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.471853\n",
      "episode 57000 completed, nsteps: 6, total discounted reward: -1.14703, running mean: -0.469567\n",
      "episode 58000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.468332\n",
      "episode 59000 completed, nsteps: 8, total discounted reward: -2.13587, running mean: -0.455067\n",
      "frames of episode 60000:\n",
      "step 0\n",
      "reward: -1.0\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "G █ ░ ☺\n",
      "░ ░ ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "Game over!\n",
      "episode 60000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.464783\n",
      "episode 61000 completed, nsteps: 2, total discounted reward: 0.95, running mean: -0.477271\n",
      "episode 62000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.484352\n",
      "episode 63000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.472061\n",
      "episode 64000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.437007\n",
      "episode 65000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: -0.446034\n",
      "episode 66000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: -0.458753\n",
      "episode 67000 completed, nsteps: 4, total discounted reward: 0.851495, running mean: -0.429807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 68000 completed, nsteps: 4, total discounted reward: -2.0594, running mean: -0.420423\n",
      "episode 69000 completed, nsteps: 14, total discounted reward: 0.387605, running mean: -0.453154\n",
      "frames of episode 70000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ G ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: ↑\n",
      "_____________\n",
      "Game over!\n",
      "episode 70000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.389744\n",
      "episode 71000 completed, nsteps: 1, total discounted reward: -1, running mean: -0.386064\n",
      "episode 72000 completed, nsteps: 15, total discounted reward: -2.26251, running mean: -0.445504\n",
      "episode 73000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.421882\n",
      "episode 74000 completed, nsteps: 13, total discounted reward: 0.431924, running mean: -0.450901\n",
      "episode 75000 completed, nsteps: 7, total discounted reward: -2.11704, running mean: -0.450913\n",
      "episode 76000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.448352\n",
      "episode 77000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.428771\n",
      "episode 78000 completed, nsteps: 11, total discounted reward: 0.52191, running mean: -0.440101\n",
      "episode 79000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.429985\n",
      "frames of episode 80000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ G\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ G ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: →\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 7\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ G F\n",
      "░ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "Game over!\n",
      "episode 80000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: -0.484835\n",
      "episode 81000 completed, nsteps: 3, total discounted reward: 0.9005, running mean: -0.448457\n",
      "episode 82000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.45655\n",
      "episode 83000 completed, nsteps: 5, total discounted reward: -2.07881, running mean: -0.418761\n",
      "episode 84000 completed, nsteps: 18, total discounted reward: 0.214716, running mean: -0.360727\n",
      "episode 85000 completed, nsteps: 1, total discounted reward: 1, running mean: -0.373711\n",
      "episode 86000 completed, nsteps: 12, total discounted reward: 0.476691, running mean: -0.43522\n",
      "episode 87000 completed, nsteps: 8, total discounted reward: 0.660327, running mean: -0.40535\n",
      "episode 88000 completed, nsteps: 4, total discounted reward: -2.0594, running mean: -0.469238\n",
      "episode 89000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.452424\n",
      "frames of episode 90000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: →\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ☺ G\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ G\n",
      "░ ░ ░ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 4\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: ↓\n",
      "_____________\n",
      "Game over!\n",
      "episode 90000 completed, nsteps: 5, total discounted reward: 0.80298, running mean: -0.46854\n",
      "episode 91000 completed, nsteps: 10, total discounted reward: 0.567586, running mean: -0.452605\n",
      "episode 92000 completed, nsteps: 6, total discounted reward: 0.75495, running mean: -0.448667\n",
      "episode 93000 completed, nsteps: 16, total discounted reward: 0.300292, running mean: -0.441894\n",
      "episode 94000 completed, nsteps: 12, total discounted reward: -2.20932, running mean: -0.40625\n",
      "episode 95000 completed, nsteps: 15, total discounted reward: 0.343729, running mean: -0.423749\n",
      "episode 96000 completed, nsteps: 2, total discounted reward: -2.02, running mean: -0.450607\n",
      "episode 97000 completed, nsteps: 7, total discounted reward: 0.707401, running mean: -0.429628\n",
      "episode 98000 completed, nsteps: 9, total discounted reward: 0.613723, running mean: -0.505542\n",
      "episode 99000 completed, nsteps: 5, total discounted reward: -2.07881, running mean: -0.437279\n",
      "frames of episode 100000:\n",
      "step 0\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "☺ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 1\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "☺ █ ░ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 2\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "☺ ░ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 3\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ☺ ░ ░\n",
      "action: →\n",
      "_____________\n",
      "step 4\n",
      "reward: -0.04\n",
      "░ ░ █ G\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 5\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ←\n",
      "_____________\n",
      "step 6\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ░ F\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 7\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ G\n",
      "░ █ ☺ F\n",
      "░ ░ ░ ░\n",
      "action: ↓\n",
      "_____________\n",
      "step 8\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ░ G\n",
      "░ ░ ☺ ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 9\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ░\n",
      "░ █ ☺ F\n",
      "░ ░ ░ G\n",
      "action: ↑\n",
      "_____________\n",
      "step 10\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ☺ ░\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: →\n",
      "_____________\n",
      "step 11\n",
      "reward: -0.04\n",
      "░ ░ █ E\n",
      "░ █ ░ ☺\n",
      "░ █ ░ F\n",
      "░ ░ G ░\n",
      "action: ↑\n",
      "_____________\n",
      "step 12\n",
      "reward: 1.0\n",
      "░ ░ █ ☺\n",
      "░ █ ░ ░\n",
      "░ █ ░ F\n",
      "░ ░ ░ G\n",
      "action: ↓\n",
      "_____________\n",
      "Game over!\n",
      "episode 100000 completed, nsteps: 13, total discounted reward: 0.431924, running mean: -0.43801\n"
     ]
    }
   ],
   "source": [
    "print('starting policy gradient iteration...')\n",
    "net = policy_gradient_iteration(net, e, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain policy from network: most likely action for each state\n",
    "pol = np.zeros(e.num_states, dtype=int)\n",
    "for s in range(e.num_states):\n",
    "    x = e.observation(s).reshape(-1)\n",
    "    aprob = net.evaluate(x[None, :])[0]\n",
    "    pol[s] = np.argmax(aprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration completed after 4 iterations\n",
      "optimal value function average: -0.28615611539027547\n"
     ]
    }
   ],
   "source": [
    "# reference optimal policy\n",
    "pref = mdp.policy_iteration(e.tprob, e.rewards, gamma)\n",
    "# corresponding value function\n",
    "uref = mdp.utility_from_policy(e.tprob, e.rewards, gamma, pref)\n",
    "# omit \"game over\" from average\n",
    "umean = np.mean(uref[:-1])\n",
    "print('optimal value function average:', umean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy (most likely action, for all possible ghost locations):\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "G → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ G ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → G ←\n",
      "\n",
      "↓ ↓ █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ G\n",
      "\n",
      "← ← █ E\n",
      "← █ → ↑\n",
      "G █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ G F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ G\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "G █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ G ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ E\n",
      "↓ █ → G\n",
      "↓ █ ← F\n",
      "→ → ↑ ←\n",
      "\n",
      "G ← █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ G █ E\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "\n",
      "↓ ← █ G\n",
      "↓ █ → ↑\n",
      "↓ █ ↑ F\n",
      "→ → ↑ ←\n",
      "number of deviations from reference: 16\n"
     ]
    }
   ],
   "source": [
    "# compare policy with reference\n",
    "print('policy (most likely action, for all possible ghost locations):')\n",
    "print(e.draw_policy(pol))\n",
    "print('number of deviations from reference:', np.sum((pol - pref) * e.pmask != 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
